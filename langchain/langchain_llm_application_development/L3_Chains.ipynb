{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnA0kjt47j+Js0zqoquctT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnamohanathota/GenerativeAI/blob/main/langchain/modules/L3_Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Chains in LangChain\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require **chaining** LLMs\n",
        "\n",
        "Chains allow us to combine multiple components together to create a single, coherent application.\n",
        "\n",
        "For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM."
      ],
      "metadata": {
        "id": "-iNqMU8AVU5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKK9EbYLWbAU",
        "outputId": "82c4c6f2-64e1-4286-ab70-ba4f988d21c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.234-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n",
            "  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.234 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())#Load local .env file\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "rGKJNo2JW7Fh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Chain\n",
        "\n",
        "https://python.langchain.com/docs/modules/chains/foundational/llm_chain\n",
        "\n",
        "- LLMChain is a simple chain that adds some functionality around language models\n",
        "- LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output."
      ],
      "metadata": {
        "id": "afhB0fA2inPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=0.0)\n",
        "prompt = PromptTemplate(input_variables=[\"product\"],\n",
        "                        template=\"What is a good name for a company that makes {product}?\")\n",
        "print(prompt)\n",
        "\n",
        "#We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "product = \"Cricket Bats\"\n",
        "print(chain.run(product))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvvVrbbXZRd",
        "outputId": "3ee39a8c-cd0c-4341-863e-5d62652766b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['product'] output_parser=None partial_variables={} template='What is a good name for a company that makes {product}?' template_format='f-string' validate_template=True\n",
            "\n",
            "\n",
            "Cricket Bat Crafters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If there are multiple variables, you can input them all at once using a dictionary.\n",
        "prompt = PromptTemplate(input_variables=[\"company\", \"product\"],\n",
        "                        template=\"What is a good name for a {company} that makes {product}?\")\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "imputDict = {'company' : 'ABC Company', \"product\" : \"Shoe\"}\n",
        "print(chain.run(imputDict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NsQDUnAcqKv",
        "outputId": "890134b6-a64c-4c08-b075-abd9c4385532"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['company', 'product'] output_parser=None partial_variables={} template='What is a good name for a {company} that makes {product}?' template_format='f-string' validate_template=True\n",
            "\n",
            "\n",
            "FootFashionz.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Chat model\" in an LLMChain\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"What is the best name to describe \\\n",
        "                                                    a company that makes {product}\")\n",
        "\n",
        "llm_chatmodel = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "chain = LLMChain(llm=llm_chatmodel, prompt=prompt_template)\n",
        "#chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "product = \"bags\"\n",
        "print(chain.run(product))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDqMke8RgRAr",
        "outputId": "5c01e28c-3f37-46e4-c751-c1afc58ead4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best name to describe a company that makes bags can vary depending on the specific focus or niche of the company. However, some possible options could be:\n",
            "\n",
            "1. BagCrafters\n",
            "2. BagMakers\n",
            "3. SturdyCarry\n",
            "4. StylePak\n",
            "5. TrendyTotes\n",
            "6. EliteCarriers\n",
            "7. BagGenius\n",
            "8. SecureSatchels\n",
            "9. FashionJunction\n",
            "10. VersaPack\n",
            "\n",
            "Ultimately, the name should reflect the brand identity, target market, and desired perception of the company making bags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleSequentialChain\n",
        "\n",
        "It is a type of `sequential chain` that allows you to connect multiple chains and execute them in a specific order.\n",
        "\n",
        "The simplest form of `sequential chains`, where each step has a singular input/output, and the output of one step is the input to the next.\n",
        "\n",
        "It takes care of passing the output of one chain as the input to the next chain, allowing you to easily chain multiple steps together.\n",
        "\n",
        "https://python.langchain.com/docs/modules/chains/foundational/sequential_chains"
      ],
      "metadata": {
        "id": "jBLbhTfwSnzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "JjCtYFo8Tuyi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\"What is the best name to describe \\\n",
        "    a company that makes {product}?\")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
        "\n",
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "# Chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
        "\n",
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )\n",
        "\n",
        "product = \"Cricket bats\"\n",
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "0zsarXU6VXxx",
        "outputId": "c1748ccc-8b37-4baf-a61a-561195ff6d22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\"Cricket Bat Co.\"\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mCricket Bat Co. is a leading manufacturer of high-quality cricket bats, offering exceptional performance and durability for players of all levels.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cricket Bat Co. is a leading manufacturer of high-quality cricket bats, offering exceptional performance and durability for players of all levels.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Chain\n",
        "\n",
        "https://python.langchain.com/docs/modules/chains/foundational/sequential_chains#sequential-chain\n",
        "\n",
        "Not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain.\n",
        "\n",
        "How we name the input/output variable names?  In the above example we didn't have to think about that because we were just passing the output of one chain directly as input to the next."
      ],
      "metadata": {
        "id": "OQQJKx6mY-je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "nngM0m7aaTh7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "# First Prompt\n",
        "first_prompt = PromptTemplate.from_template(\"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\")\n",
        "\n",
        "# First Chain : input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm = llm, prompt = first_prompt, output_key = \"English_Review\")\n",
        "\n",
        "# Second Prompt\n",
        "second_prompt = PromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\")\n",
        "\n",
        "# Second Chain : input = English_Review and output = summary\n",
        "chain_two = LLMChain(llm = llm, prompt=second_prompt, output_key=\"summary\")\n",
        "\n",
        "# Third Prompt\n",
        "third_prompt = PromptTemplate.from_template(\"What language is the follwing review : \\n \\n {Review}\")\n",
        "\n",
        "#Third Chain : input = Review and output = language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
        "\n",
        "review = \"Le nouvel iPhone 14 Plus dispose d'un écran Super Retina XDR surdimensionné. La plus longue durée de batterie jamais vue. Un nouvel appareil photo principal et un traitement d'image amélioré vous permettent de capturer des photos encore plus sensationnelles dans toutes sortes de conditions lumineuses, en particulier en basse lumière. Que vous filmiez en randonnée sur un sentier rocheux ou que vous couriez après vos enfants dans le parc, essayez le mode Action pour des vidéos stables à main levée. Des fonctionnalités de sécurité, y compris l'appel d'urgence SOS via satellite et la détection d'accident, demandent de l'aide lorsque vous en avez besoin.\"\n",
        "\n",
        "#Sequential Chain\n",
        "sequential_chain = SequentialChain(chains=[chain_one, chain_two, chain_three],\n",
        "                                   input_variables=[\"Review\"],\n",
        "                                   output_variables=[\"English_Review\", \"summary\", \"language\"],\n",
        "                                   verbose=True)\n",
        "\n",
        "\n",
        "sequential_chain.run(review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "xbZGd4aSawEj",
        "outputId": "aa30e24f-b8dc-48e5-c169-3a01d685aa58"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-db30764556ba>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msequential_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \"\"\"\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# Run at start to make sure this is possible/defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0m_output_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_run_output_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_output_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;34mf\"`run` not supported when there is not exactly \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;34mf\"one output key. Got {self.output_keys}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got ['English_Review', 'summary', 'language']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y38boAx-hBre",
        "outputId": "8a582cef-bbb4-4acf-a1c1-33aa0155a70f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Le nouvel iPhone 14 Plus dispose d'un écran Super Retina XDR surdimensionné. La plus longue durée de batterie jamais vue. Un nouvel appareil photo principal et un traitement d'image amélioré vous permettent de capturer des photos encore plus sensationnelles dans toutes sortes de conditions lumineuses, en particulier en basse lumière. Que vous filmiez en randonnée sur un sentier rocheux ou que vous couriez après vos enfants dans le parc, essayez le mode Action pour des vidéos stables à main levée. Des fonctionnalités de sécurité, y compris l'appel d'urgence SOS via satellite et la détection d'accident, demandent de l'aide lorsque vous en avez besoin.\",\n",
              " 'English_Review': \"The new iPhone 14 Plus features an oversized Super Retina XDR display. The longest battery life ever seen. A new main camera and improved image processing allow you to capture even more stunning photos in all kinds of lighting conditions, especially in low light. Whether you're filming while hiking on a rocky trail or chasing after your children in the park, try the Action mode for stable handheld videos. Safety features, including SOS emergency call via satellite and accident detection, call for help when you need it.\",\n",
              " 'summary': 'The review highlights the impressive features of the iPhone 14 Plus, such as its oversized Super Retina XDR display, longest battery life, improved camera capabilities for stunning photos in various lighting conditions, stable handheld videos with Action mode, and safety features like SOS emergency call and accident detection.',\n",
              " 'language': 'The following review is in French.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router Chain\n",
        "\n",
        "https://python.langchain.com/docs/modules/chains/foundational/router\n",
        "\n",
        "To create a chain that `dynamically` selects the next chain to use for a given input.\n",
        "\n",
        "Router chains are made up of two components:\n",
        "\n",
        "- The RouterChain itself (responsible for selecting the next chain to call)\n",
        "- destination_chains: chains that the router chain can route to\n",
        "\n",
        "--------------------\n",
        "\n",
        "So far we've covered the `LLM chain` and then a `sequential chain`.\n",
        "\n",
        "But what if you want to do something more complicated?\n",
        "\n",
        "A pretty common but basic operation is to route an input to a chain depending on what exactly that `input` is.\n",
        "\n",
        "A good way to imagine this is if you have multiple sub chains,\n",
        "each of which specialized for a particular type of input,\n",
        "you could have a router chain which first\n",
        "decides which subchain to pass it to and then passes it to\n",
        "that chain.\n",
        "\n",
        "For a concrete example, let's look at where we\n",
        "are routing between different types of chains depending\n",
        "on the subject that seems to come in.\n",
        "So we have here different prompts. One prompt is good for\n",
        "answering `physics questions`.\n",
        "The second prompt is good for answering `math questions`, the third for `history`, and then a fourth for `computer science`.\n",
        "\n",
        "Let's define all these prompt templates.\n",
        "\n",
        "After we have these prompt templates, we can then provide\n",
        "more information about them.\n",
        "\n",
        "We can give each one a name and then a description."
      ],
      "metadata": {
        "id": "oGFU6YANh-9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "F4N2m-8Jm_U7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Give More information for the prompts\n",
        "\n",
        "# After we have avove prompt templates, we can then provide more information about them.\n",
        "# We can give each one a name and then a description. This description for the physics one is good for answering questions about physics.\n",
        "\n",
        "# This information is going to be passed to the \"router chain\", so the router chain can decide when to use this subchain.\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]\n",
        "\n",
        "destinations = [f\"{p['name']}:{p['description']}\" for p in prompt_infos]\n",
        "\n",
        "print(destinations)\n",
        "print(type(destinations))\n",
        "\n",
        "destination_str = \"\\n\".join(destinations)\n",
        "print(destination_str)\n",
        "print(type(destination_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvhZUZuvnDCz",
        "outputId": "0045f35e-1db4-485d-aa65-48a850bed0ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['physics:Good for answering questions about physics', 'math:Good for answering math questions', 'History:Good for answering history questions', 'computer science:Good for answering computer science questions']\n",
            "<class 'list'>\n",
            "physics:Good for answering questions about physics\n",
            "math:Good for answering math questions\n",
            "History:Good for answering history questions\n",
            "computer science:Good for answering computer science questions\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "destination_chains = {}\n",
        "\n",
        "# each destination chain itself is a language model chain, an \"LLM chain\".\n",
        "for p_info in prompt_infos:\n",
        "  name = p_info[\"name\"]\n",
        "  prompt_template = p_info[\"prompt_template\"]\n",
        "  prompt = PromptTemplate.from_template(template = prompt_template)\n",
        "  chain = LLMChain(llm = llm, prompt=prompt)\n",
        "  destination_chains[name] = chain\n",
        "\n",
        "# Don't print this as it contains OpernAI key\n",
        "# print(destination_chains)\n",
        "print(type(destination_chains))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAJcwGpYnIj-",
        "outputId": "3d6f4d50-18ed-4e1d-c31b-8de1724030fc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now import the other types of chains that we need.\n",
        "# Here we need a \"multi-prompt\" chain. This is a specific type of chain that is used when routing\n",
        "# between multiple different prompt templates.\n",
        "\n",
        "# As you can see, all the options we have are \"prompt templates\" themselves.\n",
        "# But this is just one type of thing that you can route between. You can route between any type of chain.\n",
        "\n",
        "from langchain.chains.router import MultiPromptChain\n"
      ],
      "metadata": {
        "id": "9F5nu5MIsYCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The other classes that we'll implement here are an \"LLM router chain\".\n",
        "# This uses a language model itself to route between the different subchains.\n",
        "# This is where the description and the name provided above will be used.\n",
        "\n",
        "# We'll also import a \"router output parser\". This parses the LLM output into a dictionary that can be used downstream to determine which\n",
        "# chain to use and what the input to that chain should be.\n",
        "\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"
      ],
      "metadata": {
        "id": "igYuKXHdxwBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Default Prompt and Chain :\n",
        "\n",
        "  In addition to the destination chains, we also need a default chain.\n",
        "  This is the chain that's called when the router can't decide which of the subchains to use.\n",
        "\n",
        "  In the example above, this might be called when the input question has nothing to do with physics, math, history, or computer science.\n",
        "\"\"\"\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "foeEMZFuyq4O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we define the template that is used by the LLM to route between the different chains.\n",
        "\n",
        "# This has instructions of the task to be done, as well as the specific formatting that the output should be in.\n",
        "# Let's put a few of these pieces together to build the router chain.\n",
        "\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wwOZIhWzd5l",
        "outputId": "98749740-cbf5-4d11-b07b-1ee944bcf30a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\n",
            "\n",
            "<< FORMATTING >>\n",
            "Return a markdown code snippet with a JSON object formatted to look like:\n",
            "```json\n",
            "{{{{\n",
            "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "}}}}\n",
            "```\n",
            "\n",
            "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
            "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
            "\n",
            "<< CANDIDATE PROMPTS >>\n",
            "{destinations}\n",
            "\n",
            "<< INPUT >>\n",
            "{{input}}\n",
            "\n",
            "<< OUTPUT (remember to include the ```json)>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the full router template by formatting it with the destinations that we defined above.\n",
        "# This template is flexible to a bunch of different types of destinations.\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations = destination_str)\n",
        "print(router_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdlrjm7p1mtZ",
        "outputId": "dc9f845b-7464-4685-d4c8-061b699aeae4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\n",
            "\n",
            "<< FORMATTING >>\n",
            "Return a markdown code snippet with a JSON object formatted to look like:\n",
            "```json\n",
            "{{\n",
            "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "}}\n",
            "```\n",
            "\n",
            "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
            "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
            "\n",
            "<< CANDIDATE PROMPTS >>\n",
            "physics:Good for answering questions about physics\n",
            "math:Good for answering math questions\n",
            "History:Good for answering history questions\n",
            "computer science:Good for answering computer science questions\n",
            "\n",
            "<< INPUT >>\n",
            "{input}\n",
            "\n",
            "<< OUTPUT (remember to include the ```json)>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prompt template from above router template, and then we create the router chain by\n",
        "# passing in the LLM and the overall router prompt.\n",
        "\n",
        "# Note that here we have the router output parser. This is important as it will help this chain decide which subchains to route between.\n",
        "router_prompt_template = PromptTemplate(template=router_template,\n",
        "                              input_variables=[\"input\"],\n",
        "                              output_parser=RouterOutputParser())\n",
        "\n",
        "print(router_prompt_template)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0)\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt_template)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfm_XS5S4AsZ",
        "outputId": "f42e8492-683d-4a1e-e3b6-455603deb950"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['input'] output_parser=RouterOutputParser(default_destination='DEFAULT', next_inputs_type=<class 'str'>, next_inputs_inner_key='input') partial_variables={} template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics:Good for answering questions about physics\\nmath:Good for answering math questions\\nHistory:Good for answering history questions\\ncomputer science:Good for answering computer science questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (remember to include the ```json)>>' template_format='f-string' validate_template=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Putting it all together, we can create the overall chain.\n",
        "This has\n",
        "- router chain,\n",
        "- destination chains,\n",
        "- default chain.\n",
        "\"\"\"\n",
        "\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "TBLhAfEm5vZV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If we ask it a question about physics, we should hopefully see that it is routed to the \"physics chain\"\n",
        "with the input, what is blackbody radiation?\n",
        "\n",
        "And then that is passed into the chain down below, and we can see that the response is very detailed with lots of physics details.\n",
        "\"\"\"\n",
        "\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "P3yICDba6Ouh",
        "outputId": "5e80ac4e-a528-48dd-9f36-e4934528385f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation and reflects or transmits none. It is called \"black body\" because it absorbs all wavelengths of light, appearing black at room temperature. \\n\\nAccording to Planck\\'s law, black body radiation is characterized by a continuous spectrum of wavelengths and intensities, which depend on the temperature of the object. As the temperature increases, the peak intensity of the radiation shifts to shorter wavelengths, resulting in a change in color from red to orange, yellow, white, and eventually blue at very high temperatures.\\n\\nBlack body radiation is a fundamental concept in physics and has significant applications in various fields, including astrophysics, thermodynamics, and quantum mechanics. It played a crucial role in the development of quantum theory and understanding the behavior of light and matter.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "d0ouSSYB6dx4",
        "outputId": "f12189b5-3e62-437c-ea66-646e7c6a6a2c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank you for your kind words! As a mathematician, I am happy to help with any math questions, no matter how simple or complex they may be.\\n\\nThe question you\\'ve asked is a basic addition problem: \"What is 2 + 2?\" To solve this, we can simply add the two numbers together:\\n\\n2 + 2 = 4\\n\\nTherefore, the answer to the question \"What is 2 + 2?\" is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "So here, we ask it a question about biology and we can see the chain that it chooses is none.\n",
        "This means that it will be passed to the default chain which itself is just a generic call to the language model.\n",
        "The language model luckily knows a lot about biology so it can help us out here.\n",
        "\"\"\"\n",
        "\n",
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "8Ng7Espd6ml8",
        "outputId": "e30cdde9-e3e8-4484-aa65-1ab6ea9eb8bd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every cell in our body contains DNA because DNA is the genetic material that carries the instructions for the development, functioning, and reproduction of all living organisms. DNA contains the information necessary for the synthesis of proteins, which are essential for the structure and function of cells. It serves as a blueprint for the production of specific proteins that determine the characteristics and traits of an organism. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next during reproduction. Therefore, every cell in our body contains DNA to ensure the proper functioning and continuity of life.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}