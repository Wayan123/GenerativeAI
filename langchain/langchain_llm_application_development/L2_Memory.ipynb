{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1uvvpTiUy8yA",
        "T6vzgWLX1uY_",
        "sy3w4lQMSmXs"
      ],
      "authorship_tag": "ABX9TyP5NTX+EpyZrnVAosTVkWVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnamohanathota/GenerativeAI/blob/main/langchain/modules/L2_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jPHogYnstdri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Memory Module\n",
        "\n",
        "https://python.langchain.com/docs/modules/memory/\n",
        "\n",
        "By default, Chains and Agents are stateless, meaning that they treat each incoming query independently (like the underlying LLMs and chat models themselves). In some applications, like chatbots, it is essential to remember previous interactions, both in the short and long-term. The **Memory** class does exactly that.\n",
        "\n",
        "LangChain provides memory components in two forms:\n",
        "- Provides helper utilities for managing and manipulating previous chat messages\n",
        "- Provides easy ways to incorporate these utilities into chains\n",
        "\n",
        "Few memory types:\n",
        "- ChatMessageHistory\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationTokenBufferMemory\n",
        "- ConversationSummaryBufferMemory\n",
        "\n",
        "LangChain actually supports additional memory types as well.\n",
        "One of the most powerful is **vector** data memory,\n",
        "if you're familiar with word embeddings and text embeddings, the\n",
        "vector database actually stores such embeddings.\n"
      ],
      "metadata": {
        "id": "posHr0cltekk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65fNVT2BuXys",
        "outputId": "981ba103-83fa-485c-8754-3e859a0c92f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.233-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n",
            "  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.233 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv()) # Read local .env files\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "GJF4IclSxcTr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatMessageHistory\n",
        "\n",
        "Lightweight wrapper which exposes convenience methods for saving **Human messages**, **AI messages**, and then fetching them all."
      ],
      "metadata": {
        "id": "1uvvpTiUy8yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory();\n",
        "\n",
        "history.add_user_message(\"Hi..\")\n",
        "history.add_ai_message(\"Whats up ?\")\n",
        "\n",
        "print(history)\n",
        "print(history.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6snDDYxoy22b",
        "outputId": "ec4c5781-8fbf-4ef8-dc11-8718df316e99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Hi..', additional_kwargs={}, example=False), AIMessage(content='Whats up ?', additional_kwargs={}, example=False)]\n",
            "[HumanMessage(content='Hi..', additional_kwargs={}, example=False), AIMessage(content='Whats up ?', additional_kwargs={}, example=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationBufferMemory\n",
        "\n",
        "wrapper around **ChatMessageHistory** that extracts the messages in a variable."
      ],
      "metadata": {
        "id": "T6vzgWLX1uY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "print(memory)\n",
        "\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi!\")\n",
        "memory.chat_memory.add_ai_message(\"Whats up ???\")\n",
        "\n",
        "print(memory)\n",
        "\n",
        "## {'history': 'Human: Hi!\\nAI: Whats up ???'}\n",
        "print(memory.load_memory_variables({}))\n",
        "\n",
        "print(\"------\")\n",
        "\n",
        "## Get the history as a list of messages\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "print(memory)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi!\")\n",
        "memory.chat_memory.add_ai_message(\"Whats up ???\")\n",
        "\n",
        "print(memory)\n",
        "\n",
        "## {'history': [HumanMessage(content='Hi!', additional_kwargs={}, example=False), AIMessage(content='Whats up ???', additional_kwargs={}, example=False)]}\n",
        "print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QooP2yQk1-X0",
        "outputId": "8b860cb4-2c58-407e-c664-dced09e6c5f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_memory=ChatMessageHistory(messages=[]) output_key=None input_key=None return_messages=False human_prefix='Human' ai_prefix='AI' memory_key='history'\n",
            "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi!', additional_kwargs={}, example=False), AIMessage(content='Whats up ???', additional_kwargs={}, example=False)]) output_key=None input_key=None return_messages=False human_prefix='Human' ai_prefix='AI' memory_key='history'\n",
            "{'history': 'Human: Hi!\\nAI: Whats up ???'}\n",
            "------\n",
            "chat_memory=ChatMessageHistory(messages=[]) output_key=None input_key=None return_messages=True human_prefix='Human' ai_prefix='AI' memory_key='history'\n",
            "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi!', additional_kwargs={}, example=False), AIMessage(content='Whats up ???', additional_kwargs={}, example=False)]) output_key=None input_key=None return_messages=True human_prefix='Human' ai_prefix='AI' memory_key='history'\n",
            "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, example=False), AIMessage(content='Whats up ???', additional_kwargs={}, example=False)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using in a Chain"
      ],
      "metadata": {
        "id": "xc8oNk_c5xxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0.0)\n",
        "memory = ConversationBufferMemory();\n",
        "print(llm)\n",
        "\n",
        "# setting \"verbose=True\" so we can see the prompt\n",
        "conversation = ConversationChain(llm=llm, verbose=True, memory = memory)\n",
        "\n",
        "## Don't print \"conversation\" as this is having OpenAI API KEY\n",
        "\n",
        "conversation.predict(input=\"Hi there\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "pxdxi-xA53tV",
        "outputId": "6264af20-1067-469f-fa8a-a457efd375c9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mOpenAI\u001b[0m\n",
            "Params: {'model_name': 'text-davinci-003', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hi there! How can I help you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I'm doing well! Which country won more FIFA worldcups ? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "zOJUzbWB7QK7",
        "outputId": "fa94f3a0-1b69-4cd0-b550-32acccead0f1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi there! How can I help you?\n",
            "Human: I'm doing well! Which country won more FIFA worldcups ? \n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Who is the most popular player?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "jZMegkuC85c_",
        "outputId": "b0f672eb-3cb7-4701-f46d-c8a3c68c8ea6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-CooeaBVDf4SEqZQxXd2jJzfA on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi there! How can I help you?\n",
            "Human: I'm doing well! Which country won more FIFA worldcups ? \n",
            "AI:  Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.\n",
            "Human: Who is the most popular player?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Who had won most awards?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "eqR-lCJq9FX7",
        "outputId": "5b71c901-6d54-4c45-a202-64244dfe17d2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-CooeaBVDf4SEqZQxXd2jJzfA on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi there! How can I help you?\n",
            "Human: I'm doing well! Which country won more FIFA worldcups ? \n",
            "AI:  Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.\n",
            "Human: Who is the most popular player?\n",
            "AI:  The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\n",
            "Human: Who had won most awards?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-CooeaBVDf4SEqZQxXd2jJzfA on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-CooeaBVDf4SEqZQxXd2jJzfA on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(conversation.memory.chat_memory))\n",
        "print(type(memory.chat_memory))\n",
        "\n",
        "print(conversation.memory.chat_memory)\n",
        "print(memory.chat_memory)\n",
        "\n",
        "print(conversation.memory.chat_memory.messages)\n",
        "print(type(conversation.memory.chat_memory.messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRY5BrME-IjY",
        "outputId": "8d94f7e7-0336-4b9d-a178-4e37f972d525"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain.memory.chat_message_histories.in_memory.ChatMessageHistory'>\n",
            "<class 'langchain.memory.chat_message_histories.in_memory.ChatMessageHistory'>\n",
            "messages=[HumanMessage(content='Hi there', additional_kwargs={}, example=False), AIMessage(content=' Hi there! How can I help you?', additional_kwargs={}, example=False), HumanMessage(content=\"I'm doing well! Which country won more FIFA worldcups ? \", additional_kwargs={}, example=False), AIMessage(content=' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.', additional_kwargs={}, example=False), HumanMessage(content='Who is the most popular player?', additional_kwargs={}, example=False), AIMessage(content=\" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\", additional_kwargs={}, example=False), HumanMessage(content='Who had won most awards?', additional_kwargs={}, example=False), AIMessage(content=\" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\", additional_kwargs={}, example=False)]\n",
            "messages=[HumanMessage(content='Hi there', additional_kwargs={}, example=False), AIMessage(content=' Hi there! How can I help you?', additional_kwargs={}, example=False), HumanMessage(content=\"I'm doing well! Which country won more FIFA worldcups ? \", additional_kwargs={}, example=False), AIMessage(content=' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.', additional_kwargs={}, example=False), HumanMessage(content='Who is the most popular player?', additional_kwargs={}, example=False), AIMessage(content=\" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\", additional_kwargs={}, example=False), HumanMessage(content='Who had won most awards?', additional_kwargs={}, example=False), AIMessage(content=\" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\", additional_kwargs={}, example=False)]\n",
            "[HumanMessage(content='Hi there', additional_kwargs={}, example=False), AIMessage(content=' Hi there! How can I help you?', additional_kwargs={}, example=False), HumanMessage(content=\"I'm doing well! Which country won more FIFA worldcups ? \", additional_kwargs={}, example=False), AIMessage(content=' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.', additional_kwargs={}, example=False), HumanMessage(content='Who is the most popular player?', additional_kwargs={}, example=False), AIMessage(content=\" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\", additional_kwargs={}, example=False), HumanMessage(content='Who had won most awards?', additional_kwargs={}, example=False), AIMessage(content=\" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\", additional_kwargs={}, example=False)]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Message History\n",
        "\n",
        "You may often have to save messages, and then load them to use again. This can be done easily by first converting the messages to normal python dictionaries, saving those (as json or something) and then loading those"
      ],
      "metadata": {
        "id": "XxCQwgIG_MnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "\n",
        "dict = messages_to_dict(conversation.memory.chat_memory.messages)\n",
        "\n",
        "print(dict)\n",
        "print(type(dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViveezZB_P3f",
        "outputId": "5cddfcd7-434a-4583-f41b-013a3fbd9b8e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'type': 'human', 'data': {'content': 'Hi there', 'additional_kwargs': {}, 'example': False}}, {'type': 'ai', 'data': {'content': ' Hi there! How can I help you?', 'additional_kwargs': {}, 'example': False}}, {'type': 'human', 'data': {'content': \"I'm doing well! Which country won more FIFA worldcups ? \", 'additional_kwargs': {}, 'example': False}}, {'type': 'ai', 'data': {'content': ' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.', 'additional_kwargs': {}, 'example': False}}, {'type': 'human', 'data': {'content': 'Who is the most popular player?', 'additional_kwargs': {}, 'example': False}}, {'type': 'ai', 'data': {'content': \" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\", 'additional_kwargs': {}, 'example': False}}, {'type': 'human', 'data': {'content': 'Who had won most awards?', 'additional_kwargs': {}, 'example': False}}, {'type': 'ai', 'data': {'content': \" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\", 'additional_kwargs': {}, 'example': False}}]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_messages = messages_from_dict(dict)\n",
        "\n",
        "print(new_messages)\n",
        "print(type(new_messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhzbTpaDAVEM",
        "outputId": "29607cb3-736c-484b-d122-e4ec57ff0b7f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Hi there', additional_kwargs={}, example=False), AIMessage(content=' Hi there! How can I help you?', additional_kwargs={}, example=False), HumanMessage(content=\"I'm doing well! Which country won more FIFA worldcups ? \", additional_kwargs={}, example=False), AIMessage(content=' Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.', additional_kwargs={}, example=False), HumanMessage(content='Who is the most popular player?', additional_kwargs={}, example=False), AIMessage(content=\" The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\", additional_kwargs={}, example=False), HumanMessage(content='Who had won most awards?', additional_kwargs={}, example=False), AIMessage(content=\" The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\", additional_kwargs={}, example=False)]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bp7kfoBMCcL",
        "outputId": "166fe68e-1fe7-49ae-82ae-67ecc2fcfac5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi there\n",
            "AI:  Hi there! How can I help you?\n",
            "Human: I'm doing well! Which country won more FIFA worldcups ? \n",
            "AI:  Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.\n",
            "Human: Who is the most popular player?\n",
            "AI:  The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\n",
            "Human: Who had won most awards?\n",
            "AI:  The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Return history buffer. {} --> This means empty dicto\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IogC_pelNvKZ",
        "outputId": "23a0c83a-902c-4127-82bd-c08ad1b9b22a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi there\\nAI:  Hi there! How can I help you?\\nHuman: I'm doing well! Which country won more FIFA worldcups ? \\nAI:  Brazil has won the most FIFA World Cups, with a total of five titles. They won in 1958, 1962, 1970, 1994, and 2002.\\nHuman: Who is the most popular player?\\nAI:  The most popular player in the world is Lionel Messi. He has won the Ballon d'Or six times, and is widely considered to be one of the greatest players of all time.\\nHuman: Who had won most awards?\\nAI:  The player who has won the most awards is Cristiano Ronaldo. He has won five Ballon d'Or awards, four European Golden Shoes, and numerous other awards.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you would like to add a data manually to memory\n",
        "memory1 = ConversationBufferMemory();\n",
        "memory1.save_context({\"input\" : \"Hi\"},\n",
        "          {\"output\":\"What's up?\"})\n",
        "memory1.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uOK_JvvwQpZC",
        "outputId": "048aa77c-32a4-45c8-ff89-c9a9b19f7b7a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: Hi\\nAI: What's up?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKN3rMURRR2J",
        "outputId": "ecc380cc-c91e-4a72-ca2e-0c751aad5b5f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory1.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6kjOJv7SRhN_",
        "outputId": "212fdbab-4631-40da-c936-40b6786d8e91"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: Hi\\nAI: What's up?\\nHuman: Not much, just hanging\\nAI: Cool\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WeEUZS5RqMU",
        "outputId": "de7196bc-803d-4844-abd7-0c65c02f49e6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up?\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationBufferWindowMemory\n",
        "\n",
        "It is a type of memory that stores a fixed-size window of the conversation history. It keeps track of the most recent messages in the conversation and discards older messages as new ones are added. This memory type is useful when you only need to remember a limited amount of context from the conversation."
      ],
      "metadata": {
        "id": "sy3w4lQMSmXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "DfofPnjUUhNJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "memory.save_context({\"input\" : \"Hi\"},\n",
        "          {\"output\":\"What's up?\"})\n",
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50EoGB5JVTUU",
        "outputId": "79f93620-1569-4b89-cd2f-3b926a1f4f6a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi', additional_kwargs={}, example=False),\n",
              " AIMessage(content=\"What's up?\", additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YrJY_T8VpFI",
        "outputId": "863631ef-5d20-464e-8329-850c52c55f18"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi', additional_kwargs={}, example=False),\n",
              " AIMessage(content=\"What's up?\", additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Not much, just hanging', additional_kwargs={}, example=False),\n",
              " AIMessage(content='Cool', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUi6IiLBV1hq",
        "outputId": "841cdca8-e65a-4090-eab6-b6c4fe1f3987"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "llm = OpenAI(temperature=0.0)\n",
        "\n",
        "#If I set memory to conversational buffer window memory with k equals one,\n",
        "#the variable k equals one specifies that I wanted to remember just\n",
        "#one conversational exchange.\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "# setting \"verbose=True\" so we can see the prompt\n",
        "conversation = ConversationChain(llm=llm, verbose=True, memory = memory)\n",
        "\n",
        "conversation.predict(input = \"Hi, My name is Krishna Mohan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "gdGiEedtV-Mj",
        "outputId": "0ba7963e-ead8-49d1-b8ec-c669a5afa660"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, My name is Krishna Mohan\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi Krishna Mohan, my name is AI. It's nice to meet you. What can I do for you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "mL5QDxj5e2JX",
        "outputId": "308946a2-e8c5-4fac-eb15-4c98f4636631"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, My name is Krishna Mohan\n",
            "AI:  Hi Krishna Mohan, my name is AI. It's nice to meet you. What can I do for you today?\n",
            "Human: What is my name ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is Krishna Mohan. Is there anything else I can help you with?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is 1 + 1 ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "jURF7bcDe8WY",
        "outputId": "ff64b30c-6bdb-4120-fd32-dad7bcd7fa8d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name ?\n",
            "AI:  Your name is Krishna Mohan. Is there anything else I can help you with?\n",
            "Human: What is 1 + 1 ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 1 + 1 is equal to 2. Is there anything else I can help you with?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here it won't be able to return the name as window size was set to \"1\" for ConversationBufferWindowMemory\n",
        "# Because k equals 1, it only remembers the  last exchange versus what is 1 + 1?\n",
        "# The answer is 1 + 1 equals 2, and it's forgotten this early exchange which is now, now says, \"I'm sorry, I don't know your name.\"\n",
        "conversation.predict(input = \"What is my name ?\")\n",
        "\n",
        "# Observe the below outout, old memory has dropped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Mxa8Wa0ofAvz",
        "outputId": "c7ed2c66-04cc-40d3-f852-b11a4cc60cee"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 1 + 1 ?\n",
            "AI:  1 + 1 is equal to 2. Is there anything else I can help you with?\n",
            "Human: What is my name ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry, I don't know your name.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X88TxkcqfQkb",
        "outputId": "c115edef-f0bd-4f89-ce24-4bef614547c7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.memory.buffer_window.ConversationBufferWindowMemory"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#But memory buffer has all the history. Noy sure why ????\n",
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PfoZfViqQWh",
        "outputId": "a2e2403d-974e-4c29-8bee-c9a442607c1b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, My name is Krishna Mohan', additional_kwargs={}, example=False),\n",
              " AIMessage(content=\" Hi Krishna Mohan, my name is AI. It's nice to meet you. What can I do for you today?\", additional_kwargs={}, example=False),\n",
              " HumanMessage(content='What is my name ?', additional_kwargs={}, example=False),\n",
              " AIMessage(content=' Your name is Krishna Mohan. Is there anything else I can help you with?', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='What is 1 + 1 ?', additional_kwargs={}, example=False),\n",
              " AIMessage(content=' 1 + 1 is equal to 2. Is there anything else I can help you with?', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='What is my name ?', additional_kwargs={}, example=False),\n",
              " AIMessage(content=\" I'm sorry, I don't know your name.\", additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationTokenBufferMemory\n",
        "\n",
        "https://python.langchain.com/docs/modules/memory/how_to/token_buffer\n",
        "\n",
        "It keeps a buffer of recent interactions in memory, and uses **token length** rather than number of interactions to determine when to flush interactions.\n",
        "\n",
        "It is a type of memory that can be used to store conversation tokens in a conversation chain. It allows the conversation to be **resumed** from a specific point by providing the conversation tokens as input.\n",
        "\n",
        "It will store the conversation tokens generated during the conversation and allow you to continue the conversation from where you left off. This can be useful when you want to **resume** a conversation after a break or when you want to have a back-and-forth conversation with the AI."
      ],
      "metadata": {
        "id": "nFobPmJDrA-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ShAOQ4QuYoU",
        "outputId": "cd365b17-c7d3-4674-8b6b-5905d19dc8d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "llm = OpenAI(temperature=0.0)\n",
        "# To get the history as a list of messages\n",
        "memory = ConversationTokenBufferMemory(llm = llm, max_token_limit=30, return_messages=True)\n",
        "#memory = ConversationTokenBufferMemory(llm = llm, max_token_limit=30)\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "\n",
        "memory.load_memory_variables({})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK1rFygVq94k",
        "outputId": "3e3c08dc-a629-4dc6-a957-a7a9e520948c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [AIMessage(content='Amazing!', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='Backpropagation is what?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='Beautiful!', additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='Chatbots are what?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='Charming!', additional_kwargs={}, example=False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation_with_summary = ConversationChain(llm = llm, memory=memory, verbose=True)\n",
        "\n",
        "conversation_with_summary.predict(input=\"Hi, how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "4BKElaqvktex",
        "outputId": "2be9e26b-173c-4f25-f6a9-cce0ef792674"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[AIMessage(content='Amazing!', additional_kwargs={}, example=False), HumanMessage(content='Backpropagation is what?', additional_kwargs={}, example=False), AIMessage(content='Beautiful!', additional_kwargs={}, example=False), HumanMessage(content='Chatbots are what?', additional_kwargs={}, example=False), AIMessage(content='Charming!', additional_kwargs={}, example=False)]\n",
            "Human: Hi, how are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! I'm doing great, thanks for asking. How about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input = \"Who is the founder of OpenAI ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "6wTQTyvKldlQ",
        "outputId": "8d984444-1ee6-4bac-a5db-e9fae80124a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, how are you?', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm doing great, thanks for asking. How about you?\", additional_kwargs={}, example=False)]\n",
            "Human: Who is the founder of OpenAI ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The founder of OpenAI is Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, and Wojciech Zaremba.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"What is the use with LangChain framework?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "itGraMA7mXho",
        "outputId": "45ae667e-3fa9-40f1-80f4-ca37c91006cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: What is the use with LangChain framework?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The LangChain framework is a blockchain-based platform that enables developers to create decentralized applications (dApps) that are powered by natural language processing (NLP). It provides a secure, reliable, and scalable infrastructure for developers to build applications that can interact with users in natural language. It also provides a set of tools and services to help developers create and deploy their applications quickly and easily.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please observe above response. some histiory (tokens) are removed"
      ],
      "metadata": {
        "id": "dJ0MIF2Dm-a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationSummaryBufferMemory\n",
        "\n",
        "https://python.langchain.com/docs/modules/memory/how_to/summary_buffer\n",
        "\n",
        "- It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions, it compiles them into a summary\n",
        "- It uses token length rather than number of interactions to determine when to flush interactions."
      ],
      "metadata": {
        "id": "Jj7udo6inNV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\""
      ],
      "metadata": {
        "id": "a7jHTRZanhCY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0)\n",
        "memory = ConversationSummaryBufferMemory(llm = llm, max_token_limit=30, return_messages=True)\n",
        "\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1M2Z-KdomHD",
        "outputId": "f29db89b-604e-4550-9e02-7d08bc206071"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='\\nThe human greets the AI and asks what is on the schedule for the day. The AI responds that there is a meeting with the product team at 8am, followed by time to work on the LangChain project, and then lunch with a customer at a nearby Italian restaurant to discuss the latest in AI. The human should bring their laptop to show the latest LLM demo.', additional_kwargs={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#memory\n",
        "#Please don't print this as it is printing OpenAI key also"
      ],
      "metadata": {
        "id": "nbY_uHHZqi0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using predict_new_summary diretcly\n",
        "messages = memory.chat_memory.messages\n",
        "previous_summary = \"\"\n",
        "memory.predict_new_summary(messages, previous_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "i05JdcLtqv_D",
        "outputId": "f3d18575-be09-46c1-c428-d640635a214c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0.0)\n",
        "memory = ConversationSummaryBufferMemory(llm = llm, max_token_limit=30, return_messages=True)\n",
        "conversation_with_summary = ConversationChain(llm = llm, memory = memory, verbose=True)\n",
        "\n",
        "conversation_with_summary.predict(input = \"Hi, How are you? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "svqn1SY4r1Xg",
        "outputId": "b9c297c4-8dfc-4a60-c14d-8b9c02150fd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, How are you? \n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! I'm doing great, thanks for asking. How about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "5x03PGbPtLL4",
        "outputId": "f781a13c-5bc4-48bb-8562-f4f14d604cf2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, How are you? ', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm doing great, thanks for asking. How about you?\", additional_kwargs={}, example=False)]\n",
            "Human: Just working on writing some documentation!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' That sounds like a great use of your time! What kind of documentation are you writing?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see here that there is a summary of the conversation and then some previous interactions\n",
        "conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "ATGwPgfitRrY",
        "outputId": "46bf3812-b4b5-4b36-bbb9-201b700910b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content=\"\\nThe human asks the AI how it is doing, to which the AI responds that it is doing great and inquires about the human's wellbeing.\", additional_kwargs={}), HumanMessage(content='Just working on writing some documentation!', additional_kwargs={}, example=False), AIMessage(content=' That sounds like a great use of your time! What kind of documentation are you writing?', additional_kwargs={}, example=False)]\n",
            "Human: For LangChain! Have you heard of it?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" No, I haven't heard of LangChain. Could you tell me more about it?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see here that there is a summary of the conversation and then some previous interactions\n",
        "conversation_with_summary.predict(input=\"LangChain is a `framework` for developing applications powered by language models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "HR0txpU7td6x",
        "outputId": "89bd897b-67fa-42a7-e5fa-4ec8430f9312"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content=\"\\nThe human asks the AI how it is doing, to which the AI responds that it is doing great and inquires about the human's wellbeing. The human responds that they are working on writing documentation for LangChain, to which the AI responds positively and inquires about the kind of documentation being written.\", additional_kwargs={}), AIMessage(content=\" No, I haven't heard of LangChain. Could you tell me more about it?\", additional_kwargs={}, example=False)]\n",
            "Human: LangChain is a `framework` for developing applications powered by language models.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Interesting! What kind of applications can be developed with LangChain?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}